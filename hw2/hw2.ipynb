{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ДЗ№2. LambdaRank или LambdaMART.\n",
    "1. Нужно самостоятельно реализовать метрики ранжирования MAP, MRR, NDCG (вместо семинара).\n",
    "2. Нужно реализовать алгоритм ранжирования LambdaRank или LambdaMART. Реализовать и то и другой будет плюсом. (см. ссылки ниже).\n",
    "3. Протестировать алгоритм на «стандартных» датасэтах MQ2007 и MQ2008 https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/letor-4-0/ (вычислите метрики из пункта 1)\n",
    "4. Применить алгоритм к дастасэту movielens https://grouplens.org/datasets/movielens/ . Считая, что оценка «5» – это релевантный фильм, «4» – это тоже релевантный фильм (но очевидно релевантность таких фильмов ниже, чем с оценкой «5»), остальные оценки не являются релевантными. Подумайте как адаптировать формат из \"стандартных\" датасэтов для ранжирования к задаче рекомендаций. Не поленитесь описать это комментарием в коде.\n",
    "5. Если вы делали ДЗ №1 то сравнить, ранжирование из пункта 4 с результатами «своего» SVD, если не делали то можно сравнить результаты с «чужого» SVD (например из библиотеки https://implicit.readthedocs.io/en/latest/quickstart.html , https://surprise.readthedocs.io/en/stable/getting_started.html)\n",
    " Насколько «качество» (определите качество самостоятельно - обоснуйте выбор) отличается на топ-3, топ-5, топ-10 фильмов?\n",
    "\n",
    "Ссылки:\n",
    "1. Основная статья: https://proceedings.neurips.cc/paper/2006/file/af44c4c56f385c43f2529f9b1b018f6a-Paper.pdf (запасная ссылка: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/lambdarank.pdf )\n",
    "2. Также полезно почитать:\n",
    "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf \n",
    "https://www.cs.cmu.edu/~pinard/Papers/sigirfp092-donmez.pdf \n",
    "3. Примеры с формулами (но без подробностей):\n",
    "https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html \n",
    "4. Можно повдохновляться кодом на «плюсах»:\n",
    "https://github.com/microsoft/LightGBM/blob/e79716e0b69b27c8e6e2c39f59d0db7c63242f9a/src/objective/rank_objective.hpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MQ2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.42127860696226627\n",
      "         val_mrr            0.4961145520210266\n",
      "        val_ndcg            0.6004802179719987\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Started at 07:25:24 07-11-2023\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | RankNet | 7.5 K \n",
      "----------------------------------\n",
      "7.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n",
      "/home/ilya/repos/recsys/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "Finished at 07:32:06 07-11-2023\n",
      "Restoring states from the checkpoint path at ./logs/tb/ranknet-mq2007/version_0/checkpoints/epoch=95-step=83474.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./logs/tb/ranknet-mq2007/version_0/checkpoints/epoch=95-step=83474.ckpt\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.5218343436815923\n",
      "         val_mrr             0.643234133720398\n",
      "        val_ndcg            0.6882927890296971\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --n_epochs 100 --name ranknet-mq2007 --model ranknet --dataset mq2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: ./logs/tb/lambda-rank-mq2007\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.42127860696226627\n",
      "         val_mrr            0.4961145520210266\n",
      "        val_ndcg            0.6004802179719987\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Started at 07:32:12 07-11-2023\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | LambdaRank | 7.5 K \n",
      "-------------------------------------\n",
      "7.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n",
      "/home/ilya/repos/recsys/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "Finished at 07:39:25 07-11-2023\n",
      "Restoring states from the checkpoint path at ./logs/tb/lambda-rank-mq2007/version_0/checkpoints/epoch=77-step=67829.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./logs/tb/lambda-rank-mq2007/version_0/checkpoints/epoch=77-step=67829.ckpt\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.5230038197576488\n",
      "         val_mrr            0.6492119431495667\n",
      "        val_ndcg            0.6896548102575727\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --n_epochs 100 --name lambda-rank-mq2007 --model lambda-rank --dataset mq2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MQ2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: ./logs/tb/ranknet-mq2008\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.5442601376693492\n",
      "         val_mrr            0.6192587614059448\n",
      "        val_ndcg            0.6528293535806452\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Started at 07:39:30 07-11-2023\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | RankNet | 7.5 K \n",
      "----------------------------------\n",
      "7.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n",
      "/home/ilya/repos/recsys/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "Finished at 07:42:28 07-11-2023\n",
      "Restoring states from the checkpoint path at ./logs/tb/ranknet-mq2008/version_0/checkpoints/epoch=40-step=13886.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./logs/tb/ranknet-mq2008/version_0/checkpoints/epoch=40-step=13886.ckpt\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.6314069886356125\n",
      "         val_mrr            0.7021286487579346\n",
      "        val_ndcg            0.7241276659637561\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --n_epochs 100 --model ranknet --name ranknet-mq2008 --dataset mq2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: ./logs/tb/lambda-rank-mq2008\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.5442601376693492\n",
      "         val_mrr            0.6192587614059448\n",
      "        val_ndcg            0.6528293535806452\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Started at 07:42:32 07-11-2023\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | LambdaRank | 7.5 K \n",
      "-------------------------------------\n",
      "7.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n",
      "/home/ilya/repos/recsys/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "Finished at 07:45:39 07-11-2023\n",
      "Restoring states from the checkpoint path at ./logs/tb/lambda-rank-mq2008/version_0/checkpoints/epoch=39-step=13551.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./logs/tb/lambda-rank-mq2008/version_0/checkpoints/epoch=39-step=13551.ckpt\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.6255849091501677\n",
      "         val_mrr            0.6897296905517578\n",
      "        val_ndcg            0.7188220009685005\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --n_epochs 100 --model lambda-rank --name lambda-rank-mq2008 --dataset mq2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: ./logs/tb/ranknet-movielens\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.6568911929808794\n",
      "        val_map@3           0.6904889364739476\n",
      "        val_map@5           0.6849393290506794\n",
      "         val_mrr            0.7249204516410828\n",
      "        val_ndcg            0.7013579896975339\n",
      "       val_ndcg@3           0.7169095369436539\n",
      "       val_ndcg@5           0.7192593655426587\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Started at 07:45:44 07-11-2023\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | RankNet | 7.4 K \n",
      "----------------------------------\n",
      "7.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n",
      "/home/ilya/repos/recsys/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "Finished at 07:57:26 07-11-2023\n",
      "Restoring states from the checkpoint path at ./logs/tb/ranknet-movielens/version_0/checkpoints/epoch=52-step=49894.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./logs/tb/ranknet-movielens/version_0/checkpoints/epoch=52-step=49894.ckpt\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.7264860224960791\n",
      "        val_map@3           0.7849750178443982\n",
      "        val_map@5           0.7690503806804685\n",
      "         val_mrr             0.811733067035675\n",
      "        val_ndcg            0.7629637348127442\n",
      "       val_ndcg@3           0.7883168660434002\n",
      "       val_ndcg@5           0.7832275590317774\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --n_epochs 100 --model ranknet --name ranknet-movielens --dataset movielens --user_based True --extra_metrics True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: ./logs/tb/lambda-rank-movielens\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.6568911929808794\n",
      "        val_map@3           0.6904889364739476\n",
      "        val_map@5           0.6849393290506794\n",
      "         val_mrr            0.7249204516410828\n",
      "        val_ndcg            0.7013579896975339\n",
      "       val_ndcg@3           0.7169095369436539\n",
      "       val_ndcg@5           0.7192593655426587\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Started at 07:57:33 07-11-2023\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | LambdaRank | 7.4 K \n",
      "-------------------------------------\n",
      "7.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n",
      "/home/ilya/repos/recsys/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "Finished at 08:10:04 07-11-2023\n",
      "Restoring states from the checkpoint path at ./logs/tb/lambda-rank-movielens/version_0/checkpoints/epoch=40-step=38604.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./logs/tb/lambda-rank-movielens/version_0/checkpoints/epoch=40-step=38604.ckpt\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         val_map            0.7261057544200965\n",
      "        val_map@3           0.7818522483940057\n",
      "        val_map@5           0.7690355103497525\n",
      "         val_mrr             0.809316873550415\n",
      "        val_ndcg            0.7624941383000066\n",
      "       val_ndcg@3           0.7853824280483077\n",
      "       val_ndcg@5           0.7830316175072032\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --n_epochs 100 --model lambda-rank --name lambda-rank-movielens --dataset movielens --user_based True --extra_metrics True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def load(fname, path='/home/ilya/repos/recsys/data/ml-100k'):\n",
    "    path = os.path.join(path, fname)\n",
    "    return pd.read_csv(path, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp']).drop(columns=['timestamp'])\n",
    "\n",
    "def load_train():\n",
    "    return load('ua.base')\n",
    "def load_test():\n",
    "    return load('ua.test')\n",
    "\n",
    "df_train = load_train()\n",
    "df_test = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranking import svd\n",
    "\n",
    "hparams = dict(\n",
    "    n_factors=100,\n",
    "    n_epochs=10,\n",
    "    batch_size=128,\n",
    "    init_mean=0,\n",
    "    init_std_dev=.1,\n",
    "    biased=False,\n",
    "    lr=.03,\n",
    "    reg=.1,\n",
    "    random_state=0,\n",
    "    return_logs=False\n",
    ")\n",
    "\n",
    "df_train, df_test = svd(df_train, df_test, **hparams)\n",
    "df_test['rating'] = df_test['rating'].apply(lambda x: (x>3)*(x-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(i, dataset, user_based):\n",
    "    if user_based:\n",
    "        mask = dataset.user_id == i\n",
    "    else:\n",
    "        mask = dataset.item_id == i\n",
    "\n",
    "    batch = dataset[mask]\n",
    "    y_true = batch['rating'].to_numpy()\n",
    "    y_score = batch['pred_rating'].to_numpy()\n",
    "\n",
    "    return y_true, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = df_test.user_id.unique()\n",
    "\n",
    "targets = {}\n",
    "preds = {}\n",
    "for i in user_ids:\n",
    "    y_true, y_score = get_item(i, df_test, user_based=True)\n",
    "    if y_true.sum() == 0:\n",
    "        continue\n",
    "    targets[i] = y_true\n",
    "    preds[i] = y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10: 0.7973909197174788\n",
      "MRR: 0.8851232962849669\n",
      "NDCG@10: 0.8284893162403983\n"
     ]
    }
   ],
   "source": [
    "from ranking.metrics import MAP, MRR, NDCG\n",
    "\n",
    "print('MAP@10:', MAP(targets, preds))\n",
    "print('MRR:', MRR(targets, preds))\n",
    "print('NDCG@10:', NDCG(targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@5: 0.8429380799429005\n",
      "NDCG@5: 0.8463349558573988\n"
     ]
    }
   ],
   "source": [
    "print('MAP@5:', MAP(targets, preds, k=5))\n",
    "print('NDCG@5:', NDCG(targets, preds, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@3: 0.8626873661670256\n",
      "NDCG@3: 0.8580605118511195\n"
     ]
    }
   ],
   "source": [
    "print('MAP@3:', MAP(targets, preds, k=3))\n",
    "print('NDCG@3:', NDCG(targets, preds, k=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
